{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 17:39:25.737070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import logging, os, argparse\n",
    "\n",
    "from prompt_tuner import PromptTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this substitutes argparse in Jupyter Notebook\n",
    "class Args:\n",
    "    model_name = 't5-base'\n",
    "    task = 'boolq'\n",
    "    batch_size = 4\n",
    "    select_k_per_class = 500\n",
    "    prefix_len = 10\n",
    "    prefix_path = ''\n",
    "    freeze_weights = 1\n",
    "    freeze_except = 'none'\n",
    "    lr = 0.5\n",
    "    seq_len = 512\n",
    "    early_stopping = 1\n",
    "    prefix_MLP = 'MLP1'\n",
    "    bottleneck_size = 500\n",
    "    mlp_layer_norm = 1\n",
    "    get_test_subset = False\n",
    "\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing weights\n",
      "Using MLP reparametrization with bottleneck =  500\n",
      "Using skip connection in MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/anastasia/anaconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Reusing dataset super_glue (/h/anastasia/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d427eb5795d34c5cbbc0e9a3dfa36405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/anastasia/anaconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/h/anastasia/anaconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  500   k-val =  -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (/h/anastasia/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "Loading cached shuffled indices for dataset at /h/anastasia/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-56923150e227aa8a.arrow\n",
      "Loading cached processed dataset at /h/anastasia/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-261c94d7899cb575.arrow\n"
     ]
    }
   ],
   "source": [
    "prompt_tuner = PromptTuner(model_name=args.model_name,\n",
    "                           task=args.task,\n",
    "                           batch_size=args.batch_size,\n",
    "                           select_k_per_class=args.select_k_per_class,\n",
    "                           prefix_len=args.prefix_len,\n",
    "                           prefix_path=args.prefix_path if args.prefix_path!='' else None, # path to the pre-trained progressive prompt\n",
    "                           freeze_weights=args.freeze_weights==1,\n",
    "                           freeze_except=args.freeze_except,\n",
    "                           lr=args.lr,\n",
    "                           weight_decay=1e-5,\n",
    "                           seq_len=args.seq_len,\n",
    "                           early_stopping=args.early_stopping==1,\n",
    "                           prefix_MLP=args.prefix_MLP,\n",
    "                           bottleneck_size=args.bottleneck_size, # bottleneck size in case of using MLP reparametrization\n",
    "                           mlp_lr=None,\n",
    "                           mlp_layer_norm=args.mlp_layer_norm==1,\n",
    "                           weight_decay_mlp=None,\n",
    "                           get_test_subset=args.get_test_subset,\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResMLP(\n",
       "  (module): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=500, out_features=768, bias=True)\n",
       "    (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_tuner.prefix_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prompt_tuner.model.prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task =  boolq\n",
      "Freezing all MLPs except for  boolq\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bc55593de54009b3228e51b651c805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12331c77cc3d49debf28d33b43957de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['true', 'true', 'false', 'true']\n",
      "['false', 'true', 'true', 'false']\n",
      "['false', 'true', 'true', 'true']\n",
      "['false', 'false', 'false', 'true']\n",
      "['false', 'true', 'true', 'false']\n",
      "['true', 'true', 'true', 'false']\n",
      "['true', 'false', 'true', 'false']\n",
      "['true', 'false', 'true', 'false']\n",
      "['false', 'true', 'false', 'false']\n",
      "['false', 'false', 'false', 'false']\n",
      "['false', 'true', 'true', 'true']\n",
      "['false', 'false', 'true', 'false']\n",
      "['false', 'true', 'false', 'true']\n",
      "['false', 'true', 'false', 'true']\n",
      "['true', 'true', 'true', 'false']\n",
      "['false', 'true', 'true', 'true']\n",
      "['false', 'false', 'true', 'false']\n",
      "['false', 'false', 'true', 'true']\n",
      "['true', 'true', 'true', 'true']\n",
      "['true', 'true', 'true', 'true']\n"
     ]
    }
   ],
   "source": [
    "score_dict = prompt_tuner.train_one_task(epochs=4,\n",
    "                                         eval_every_N=1,\n",
    "                                         save_path='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': {'acc': [0.6840978593272171], 'loss': [0.3668331]},\n",
       " 'train': {'acc': [0.498, 0.565, 0.627, 0.634, 0.647],\n",
       "  'loss': [0.7972758, 0.4028198, 0.36044654, 0.34218577, 0.3419612]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
